{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ppsDZSG0OrP4"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Initialising Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c2k6nlBOrP8",
    "outputId": "5ed1087b-af57-461f-cc2e-ff3fcfe6c795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Nural Network for predicting Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OJ5sG1z3OrP_"
   },
   "outputs": [],
   "source": [
    "class NNmodel(tf.keras.Model):\n",
    "    def __init__(self,action_size):\n",
    "        super(NNmodel,self).__init__(name='')\n",
    "        self.hidden1=tf.keras.layers.Dense(100,activation = 'relu',kernel_initializer=tf.keras.initializers.he_normal())\n",
    "        \n",
    "        self.hidden3=tf.keras.layers.Dense(50,activation = 'relu',kernel_initializer=tf.keras.initializers.he_normal())\n",
    "        \n",
    "        self.output_layer=tf.keras.layers.Dense(action_size,activation = 'linear',kernel_initializer=tf.keras.initializers.he_normal())\n",
    "        self.action_size=action_size\n",
    "        \n",
    "    def call(self,state):\n",
    "        state = np.reshape(state,(-1,4))\n",
    "        \n",
    "        x = self.hidden1(state)\n",
    "        \n",
    "        x = self.hidden3(x)\n",
    "        \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tOB4oy7gOrQB"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0U7p0am8OrQD"
   },
   "outputs": [],
   "source": [
    "def loss(model, x, y , action, action_size):\n",
    "    y_ = model(x)\n",
    "    action = tf.one_hot(action, depth = action_size)\n",
    "    action = tf.reshape(action,(action_size,-1))\n",
    "    y_= tf.reduce_sum(tf.matmul(y_,action),axis=1)\n",
    "    return loss_object(y_true=y, y_pred=y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nchmBq0MOrQF"
   },
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets , action, action_size):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs,targets, action, action_size)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Replay Buffer for memorising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "31awEpEmOrQI"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        samples = random.choices(self.buffer, k=sample_size)\n",
    "        return map(list, zip(*samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xTfunQ0COrQK"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,env):\n",
    "        if(type(env.action_space) == gym.spaces.discrete.Discrete):\n",
    "            self.is_discrete = True\n",
    "        else:\n",
    "            self.is_discrete = False\n",
    "        if(self.is_discrete):\n",
    "            self.action_size = env.action_space.n\n",
    "            print(\"action size\" ,self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            \n",
    "    \n",
    "    def get_action(self,state):\n",
    "        if(self.is_discrete):\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = random.uniform(self.action_low,self.action_high,self.action_shape)\n",
    "        return action    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Deep Q learning Agent for taking action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oX-SON7oOrQM"
   },
   "outputs": [],
   "source": [
    "class DQNagent(Agent):\n",
    "    def __init__(self,env,discount_rate=0.8,learning_rate=0.001):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(maxlen=10000)\n",
    "        self.eps = 1.0\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=self.learning_rate,decay_steps=10000,decay_rate=0.99)\n",
    "        self.build_agent()\n",
    "        self.optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr_schedule)\n",
    "       \n",
    "\n",
    "    def build_agent(self):\n",
    "        self.model = NNmodel(self.action_size)\n",
    "        \n",
    "        \n",
    "    def get_action(self,state):\n",
    "        \n",
    "        q_values = self.model(state)\n",
    "        q_values = q_values.numpy()\n",
    "        \n",
    "        max_ind = []\n",
    "        max_val = np.max(q_values[0])\n",
    "        for i in range(len(q_values[0])):\n",
    "            if q_values[0][i]==max_val:\n",
    "                max_ind.append(i)\n",
    "            \n",
    "        action = random.choice(max_ind)\n",
    "        \n",
    "        rand_action = super().get_action(state)\n",
    "        if random.random()<self.eps:\n",
    "            \n",
    "            return rand_action\n",
    "        else:\n",
    "            return action\n",
    "        \n",
    "    def train(self,experience):\n",
    "        state , action , next_state , reward , done ,total_reward = experience\n",
    "        self.update_weights(experience)\n",
    "        self.replay_buffer.add((state, action, next_state, reward, done , total_reward))\n",
    "        states, actions, next_states, rewards, dones , total_rewards = self.replay_buffer.sample(10)\n",
    "        for i in range(len(states)):\n",
    "            state=states[i]\n",
    "            action=actions[i]\n",
    "            next_state = next_states[i]\n",
    "            reward=rewards[i]\n",
    "            done=dones[i]\n",
    "            total_reward = total_rewards[i]\n",
    "\n",
    "            self.update_weights((state, action, next_state, reward, done,total_reward))\n",
    "            \n",
    "    def update_weights(self,experience):\n",
    "        state , action , next_state , reward , done ,total_reward= experience\n",
    "        q_next = self.model(next_state)\n",
    "        q_next = q_next.numpy()\n",
    "        \n",
    "        if total_reward>=199:\n",
    "            done = False\n",
    "        q_next[done] = np.zeros([self.action_size])\n",
    "        \n",
    "        q_target = reward + self.discount_rate * np.max(q_next)\n",
    "        loss_value, grads = grad(self.model, state, q_target,action,self.action_size)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        loss_value, grads = grad(self.model, state, q_target,action,self.action_size)\n",
    "       \n",
    "        if done: \n",
    "            self.eps *=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjxQX8-pOrQO",
    "outputId": "929e16b1-243d-444b-8980-332fcc455aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action size 2\n"
     ]
    }
   ],
   "source": [
    "agent = DQNagent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8YkM4VOOrQQ",
    "outputId": "6cc61055-5047-4bf5-8a26-862e9d4fe233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer n_nmodel_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "at episode 0 reward =30.0 mean =30.0\n",
      "at episode 20 reward =10.0 mean =13.3\n",
      "at episode 40 reward =10.0 mean =11.95\n",
      "at episode 60 reward =37.0 mean =16.683333333333334\n",
      "at episode 80 reward =61.0 mean =24.8\n",
      "at episode 100 reward =82.0 mean =32.53\n",
      "at episode 120 reward =200.0 mean =61.69\n",
      "at episode 140 reward =200.0 mean =98.05\n",
      "at episode 160 reward =200.0 mean =128.19\n",
      "at episode 180 reward =200.0 mean =153.93\n",
      "at episode 200 reward =200.0 mean =161.71\n",
      "at episode 220 reward =162.0 mean =167.64\n",
      "at episode 240 reward =200.0 mean =164.87\n",
      "at episode 260 reward =199.0 mean =165.75\n",
      "at episode 280 reward =200.0 mean =161.78\n",
      "at episode 300 reward =200.0 mean =172.35\n",
      "at episode 320 reward =200.0 mean =169.26\n",
      "at episode 340 reward =71.0 mean =169.84\n",
      "at episode 360 reward =200.0 mean =167.14\n",
      "at episode 380 reward =162.0 mean =171.75\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for ep in range(400):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward=0\n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        n_reward=reward\n",
    "        agent.train((state,action,next_state,reward,done,total_reward))\n",
    "        state = next_state\n",
    "        total_reward += n_reward\n",
    "        #env.render()\n",
    "    \n",
    "    scores.append(total_reward)\n",
    "    \n",
    "    if ep==0:\n",
    "\t\t    mean = scores[ep]\n",
    "\t  \n",
    "    if ep>0:\n",
    "\t\t    mean = np.mean(scores[np.maximum(0,ep-100):ep])#calculating rolling average\n",
    "    \n",
    "    if ep%20==0:\n",
    "        print(\"at episode {} reward ={} mean ={}\".format(ep,total_reward,mean))\n",
    "    \n",
    "    if ep>98 and mean > 180:\n",
    "        print(\"at episode {} reward ={} mean ={}\".format(ep,total_reward,mean))\n",
    "        break  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-YzZEmEOrQY"
   },
   "source": [
    "<b> average reward of last 100 steps = 171.75"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
